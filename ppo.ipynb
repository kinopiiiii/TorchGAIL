{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d282877214f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>\n",
    "16スレッドで環境を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "\n",
    "num_envs = 16\n",
    "env_name = \"Pendulum-v0\"\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(num_inputs, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic(x)\n",
    "        mu    = self.actor(x)\n",
    "        std   = self.log_std.exp().expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "    plt.plot(rewards)\n",
    "    plt.show()\n",
    "    \n",
    "def test_env(vis=True):\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        next_state, reward, done, _ = env.step(dist.sample().cpu().numpy()[0])\n",
    "        state = next_state\n",
    "        if vis: env.render()\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>GAE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
    "        gae = delta + gamma * tau * masks[step] * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proximal Policy Optimization Algorithm</h1>\n",
    "<h2><a href=\"https://arxiv.org/abs/1707.06347\">Arxiv</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
    "    batch_size = states.size(0)\n",
    "    for _ in range(batch_size // mini_batch_size):\n",
    "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
    "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
    "        \n",
    "        \n",
    "\n",
    "def ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
    "    for _ in range(ppo_epochs):\n",
    "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
    "            dist, value = model(state)\n",
    "            entropy = dist.entropy().mean()\n",
    "            new_log_probs = dist.log_prob(action)\n",
    "\n",
    "            ratio = (new_log_probs - old_log_probs).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * advantage\n",
    "\n",
    "            actor_loss  = - torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (return_ - value).pow(2).mean()\n",
    "\n",
    "            loss = 0.5 * critic_loss + actor_loss - 0.001 * entropy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 256\n",
    "lr               = 3e-4\n",
    "num_steps        = 20\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "threshold_reward = -200\n",
    "\n",
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_frames = 15000\n",
    "frame_idx  = 0\n",
    "test_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "\n",
    "while frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    log_probs = []\n",
    "    values    = []\n",
    "    states    = []\n",
    "    actions   = []\n",
    "    rewards   = []\n",
    "    masks     = []\n",
    "    entropy = 0\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        dist, value = model(state)\n",
    "\n",
    "        action = dist.sample()\n",
    "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy += dist.entropy().mean()\n",
    "        \n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state = next_state\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 1000 == 0:\n",
    "            test_reward = np.mean([test_env() for _ in range(10)])\n",
    "            test_rewards.append(test_reward)\n",
    "            plot(frame_idx, test_rewards)\n",
    "            if test_reward > threshold_reward: early_stop = True\n",
    "            \n",
    "\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    _, next_value = model(next_state)\n",
    "    returns = compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "    returns   = torch.cat(returns).detach()\n",
    "    log_probs = torch.cat(log_probs).detach()\n",
    "    values    = torch.cat(values).detach()\n",
    "    states    = torch.cat(states)\n",
    "    actions   = torch.cat(actions)\n",
    "    advantage = returns - values\n",
    "    \n",
    "    ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Saving trajectories for GAIL</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -252.975023103\n",
      "episode: 1 reward: -134.554073169\n",
      "episode: 2 reward: -367.126226518\n",
      "episode: 3 reward: -252.960441359\n",
      "episode: 4 reward: -0.801971570396\n",
      "episode: 5 reward: -4.99866743151\n",
      "episode: 6 reward: -631.510810776\n",
      "episode: 7 reward: -137.820671025\n",
      "episode: 8 reward: -546.432568631\n",
      "episode: 9 reward: -1.29977341054\n",
      "episode: 10 reward: -0.728181566692\n",
      "episode: 11 reward: -984.785845862\n",
      "episode: 12 reward: -0.834325110821\n",
      "episode: 13 reward: -894.939466634\n",
      "episode: 14 reward: -905.329185181\n",
      "episode: 15 reward: -133.391182175\n",
      "episode: 16 reward: -119.055539998\n",
      "episode: 17 reward: -254.311331827\n",
      "episode: 18 reward: -1000.14779306\n",
      "episode: 19 reward: -115.00048869\n",
      "episode: 20 reward: -254.049525013\n",
      "episode: 21 reward: -132.618270214\n",
      "episode: 22 reward: -0.817162706996\n",
      "episode: 23 reward: -1.44697820942\n",
      "episode: 24 reward: -892.559848405\n",
      "episode: 25 reward: -772.910791551\n",
      "episode: 26 reward: -1.38965858449\n",
      "episode: 27 reward: -978.306654483\n",
      "episode: 28 reward: -355.764373822\n",
      "episode: 29 reward: -267.77770897\n",
      "episode: 30 reward: -910.720014184\n",
      "episode: 31 reward: -232.973015543\n",
      "episode: 32 reward: -133.196278071\n",
      "episode: 33 reward: -252.940318824\n",
      "episode: 34 reward: -358.485861995\n",
      "episode: 35 reward: -117.857247968\n",
      "episode: 36 reward: -1.49672497533\n",
      "episode: 37 reward: -2.24599001014\n",
      "episode: 38 reward: -121.440913921\n",
      "episode: 39 reward: -133.400282112\n",
      "episode: 40 reward: -366.098082601\n",
      "episode: 41 reward: -119.129520311\n",
      "episode: 42 reward: -254.001520648\n",
      "episode: 43 reward: -244.849368235\n",
      "episode: 44 reward: -389.989592284\n",
      "episode: 45 reward: -121.525703985\n",
      "episode: 46 reward: -1026.95390139\n",
      "episode: 47 reward: -376.434491889\n",
      "episode: 48 reward: -326.725403565\n",
      "episode: 49 reward: -888.05704541\n",
      "episode: 50 reward: -878.416744524\n",
      "episode: 51 reward: -499.581253763\n",
      "episode: 52 reward: -392.195310981\n",
      "episode: 53 reward: -889.664952672\n",
      "episode: 54 reward: -134.868784002\n",
      "episode: 55 reward: -898.8998127\n",
      "episode: 56 reward: -250.050822195\n",
      "episode: 57 reward: -130.304615917\n",
      "episode: 58 reward: -867.838396921\n",
      "episode: 59 reward: -509.107713228\n",
      "episode: 60 reward: -1.99393859216\n",
      "episode: 61 reward: -246.417189892\n",
      "episode: 62 reward: -961.630590558\n",
      "episode: 63 reward: -990.100451862\n",
      "episode: 64 reward: -136.384597645\n",
      "episode: 65 reward: -897.027458887\n",
      "episode: 66 reward: -2.66714547709\n",
      "episode: 67 reward: -136.709799949\n",
      "episode: 68 reward: -251.292176086\n",
      "episode: 69 reward: -121.580063804\n",
      "episode: 70 reward: -895.431267647\n",
      "episode: 71 reward: -402.718787974\n",
      "episode: 72 reward: -900.123171564\n",
      "episode: 73 reward: -906.407196414\n",
      "episode: 74 reward: -905.315151977\n",
      "episode: 75 reward: -121.252344625\n",
      "episode: 76 reward: -473.516359946\n",
      "episode: 77 reward: -246.611373931\n",
      "episode: 78 reward: -122.082997762\n",
      "episode: 79 reward: -244.861266242\n",
      "episode: 80 reward: -115.876241336\n",
      "episode: 81 reward: -244.622987766\n",
      "episode: 82 reward: -133.657247586\n",
      "episode: 83 reward: -135.96887121\n",
      "episode: 84 reward: -246.670959984\n",
      "episode: 85 reward: -357.2620862\n",
      "episode: 86 reward: -245.761850642\n",
      "episode: 87 reward: -132.411832251\n",
      "episode: 88 reward: -0.879689999602\n",
      "episode: 89 reward: -256.885262522\n",
      "episode: 90 reward: -1041.48548168\n",
      "episode: 91 reward: -247.442439045\n",
      "episode: 92 reward: -389.851090854\n",
      "episode: 93 reward: -834.542179571\n",
      "episode: 94 reward: -847.061382641\n",
      "episode: 95 reward: -253.87049625\n",
      "episode: 96 reward: -902.598450717\n",
      "episode: 97 reward: -1.63840228832\n",
      "episode: 98 reward: -396.847655009\n",
      "episode: 99 reward: -241.933154533\n",
      "episode: 100 reward: -117.196053268\n",
      "episode: 101 reward: -116.285026726\n",
      "episode: 102 reward: -117.003228083\n",
      "episode: 103 reward: -121.728101981\n",
      "episode: 104 reward: -116.024501482\n",
      "episode: 105 reward: -846.099019868\n",
      "episode: 106 reward: -385.464678738\n",
      "episode: 107 reward: -888.494752227\n",
      "episode: 108 reward: -250.72175432\n",
      "episode: 109 reward: -132.505091513\n",
      "episode: 110 reward: -133.871999673\n",
      "episode: 111 reward: -116.434659607\n",
      "episode: 112 reward: -120.642747829\n",
      "episode: 113 reward: -877.579601534\n",
      "episode: 114 reward: -906.124968547\n",
      "episode: 115 reward: -135.892587606\n",
      "episode: 116 reward: -380.431252131\n",
      "episode: 117 reward: -885.951620743\n",
      "episode: 118 reward: -906.365442422\n",
      "episode: 119 reward: -904.100112325\n",
      "episode: 120 reward: -893.608553355\n",
      "episode: 121 reward: -385.036919543\n",
      "episode: 122 reward: -769.987044253\n",
      "episode: 123 reward: -251.970329579\n",
      "episode: 124 reward: -251.578931022\n",
      "episode: 125 reward: -257.086785824\n",
      "episode: 126 reward: -253.366776789\n",
      "episode: 127 reward: -1.4116615505\n",
      "episode: 128 reward: -256.544435072\n",
      "episode: 129 reward: -233.117331998\n",
      "episode: 130 reward: -135.818597439\n",
      "episode: 131 reward: -117.212551415\n",
      "episode: 132 reward: -119.511916436\n",
      "episode: 133 reward: -891.685370281\n",
      "episode: 134 reward: -134.223480392\n",
      "episode: 135 reward: -649.256180373\n",
      "episode: 136 reward: -1.3967107651\n",
      "episode: 137 reward: -940.04983118\n",
      "episode: 138 reward: -373.677936657\n",
      "episode: 139 reward: -0.965246298109\n",
      "episode: 140 reward: -4.48777579929\n",
      "episode: 141 reward: -2.34299213058\n",
      "episode: 142 reward: -256.944720414\n",
      "episode: 143 reward: -117.253760493\n",
      "episode: 144 reward: -131.697181931\n",
      "episode: 145 reward: -119.92914562\n",
      "episode: 146 reward: -250.221407105\n",
      "episode: 147 reward: -251.457866183\n",
      "episode: 148 reward: -475.596906632\n",
      "episode: 149 reward: -877.851568275\n",
      "episode: 150 reward: -1062.63409871\n",
      "episode: 151 reward: -874.167721881\n",
      "episode: 152 reward: -133.338466346\n",
      "episode: 153 reward: -887.782245372\n",
      "episode: 154 reward: -812.914913841\n",
      "episode: 155 reward: -787.190753337\n",
      "episode: 156 reward: -432.958174452\n",
      "episode: 157 reward: -136.973607811\n",
      "episode: 158 reward: -901.656221353\n",
      "episode: 159 reward: -135.704203436\n",
      "episode: 160 reward: -241.026418525\n",
      "episode: 161 reward: -900.782265065\n",
      "episode: 162 reward: -245.98544453\n",
      "episode: 163 reward: -136.444397908\n",
      "episode: 164 reward: -887.803970404\n",
      "episode: 165 reward: -905.948991891\n",
      "episode: 166 reward: -2.26683237305\n",
      "episode: 167 reward: -786.73316282\n",
      "episode: 168 reward: -890.897292237\n",
      "episode: 169 reward: -121.694812439\n",
      "episode: 170 reward: -957.397524607\n",
      "episode: 171 reward: -115.174712689\n",
      "episode: 172 reward: -0.965641214696\n",
      "episode: 173 reward: -371.725026147\n",
      "episode: 174 reward: -895.683029283\n",
      "episode: 175 reward: -135.469540999\n",
      "episode: 176 reward: -370.271347827\n",
      "episode: 177 reward: -775.630237651\n",
      "episode: 178 reward: -261.240441266\n",
      "episode: 179 reward: -378.329401894\n",
      "episode: 180 reward: -1.24143519321\n",
      "episode: 181 reward: -135.737760658\n",
      "episode: 182 reward: -250.752696298\n",
      "episode: 183 reward: -1.7469429641\n",
      "episode: 184 reward: -1.10667153687\n",
      "episode: 185 reward: -117.683195126\n",
      "episode: 186 reward: -116.726895561\n",
      "episode: 187 reward: -872.394067804\n",
      "episode: 188 reward: -134.916666445\n",
      "episode: 189 reward: -2.59197192111\n",
      "episode: 190 reward: -254.059622006\n",
      "episode: 191 reward: -1.48066623668\n",
      "episode: 192 reward: -888.414787149\n",
      "episode: 193 reward: -895.102958483\n",
      "episode: 194 reward: -243.93896811\n",
      "episode: 195 reward: -1.97281265307\n",
      "episode: 196 reward: -904.567168149\n",
      "episode: 197 reward: -115.185912264\n",
      "episode: 198 reward: -830.205681816\n",
      "episode: 199 reward: -119.928494997\n",
      "episode: 200 reward: -236.065202244\n",
      "episode: 201 reward: -366.145629353\n",
      "episode: 202 reward: -245.987878059\n",
      "episode: 203 reward: -272.588101961\n",
      "episode: 204 reward: -119.150971195\n",
      "episode: 205 reward: -131.075224591\n",
      "episode: 206 reward: -133.962299578\n",
      "episode: 207 reward: -251.092921524\n",
      "episode: 208 reward: -3.15615333327\n",
      "episode: 209 reward: -862.502813987\n",
      "episode: 210 reward: -252.783356443\n",
      "episode: 211 reward: -1.58496329583\n",
      "episode: 212 reward: -768.753956708\n",
      "episode: 213 reward: -883.340452257\n",
      "episode: 214 reward: -131.330407941\n",
      "episode: 215 reward: -897.449879021\n",
      "episode: 216 reward: -266.504516688\n",
      "episode: 217 reward: -1013.73505143\n",
      "episode: 218 reward: -135.012262692\n",
      "episode: 219 reward: -0.838698499195\n",
      "episode: 220 reward: -134.151874174\n",
      "episode: 221 reward: -1008.16073476\n",
      "episode: 222 reward: -910.752081418\n",
      "episode: 223 reward: -893.700879863\n",
      "episode: 224 reward: -889.367551787\n",
      "episode: 225 reward: -879.266081264\n",
      "episode: 226 reward: -251.017824477\n",
      "episode: 227 reward: -771.377220767\n",
      "episode: 228 reward: -900.831744951\n",
      "episode: 229 reward: -117.400168038\n",
      "episode: 230 reward: -0.802844611485\n",
      "episode: 231 reward: -253.255735672\n",
      "episode: 232 reward: -118.065015612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 233 reward: -136.389895277\n",
      "episode: 234 reward: -134.776478382\n",
      "episode: 235 reward: -536.850796458\n",
      "episode: 236 reward: -0.726615980331\n",
      "episode: 237 reward: -1043.94657627\n",
      "episode: 238 reward: -378.551766742\n",
      "episode: 239 reward: -988.413339117\n",
      "episode: 240 reward: -909.886497514\n",
      "episode: 241 reward: -135.882794803\n",
      "episode: 242 reward: -121.70519324\n",
      "episode: 243 reward: -1.55588912049\n",
      "episode: 244 reward: -1.70542585834\n",
      "episode: 245 reward: -239.575580752\n",
      "episode: 246 reward: -248.857300518\n",
      "episode: 247 reward: -783.322345792\n",
      "episode: 248 reward: -132.39247654\n",
      "episode: 249 reward: -116.359621069\n",
      "\n",
      "[[-0.43083966 -0.90242849 -0.71318909 -1.843346  ]\n",
      " [-0.50546737 -0.86284572 -1.69001046 -4.24799871]\n",
      " [-0.61452268 -0.78889915 -2.63714476 -3.29462194]\n",
      " ..., \n",
      " [ 0.99862814  0.05236258 -0.07437988 -0.85781854]\n",
      " [ 0.9988397   0.04815873 -0.08418329 -0.32716894]\n",
      " [ 0.99900882  0.04451256 -0.07300203 -0.16625193]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "max_expert_num = 50000\n",
    "num_steps = 0\n",
    "expert_traj = []\n",
    "\n",
    "for i_episode in count():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        dist, _ = model(state)\n",
    "        action = dist.sample().cpu().numpy()[0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        expert_traj.append(np.hstack([state, action]))\n",
    "        num_steps += 1\n",
    "    \n",
    "    print(\"episode:\", i_episode, \"reward:\", total_reward)\n",
    "    \n",
    "    if num_steps >= max_expert_num:\n",
    "        break\n",
    "        \n",
    "expert_traj = np.stack(expert_traj)\n",
    "print()\n",
    "print(expert_traj.shape)\n",
    "print()\n",
    "np.save(\"expert_traj.npy\", expert_traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
