{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Replay Buffer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done, goal):\n",
    "        self.buffer.append((state, action, reward, next_state, done, goal))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done, goal = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.stack(state), action, reward, np.stack(next_state), done, np.stack(goal)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Bit Flipping Environment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(object):\n",
    "    def __init__(self, num_bits):\n",
    "        self.num_bits = num_bits\n",
    "    \n",
    "    def reset(self):\n",
    "        self.done      = False\n",
    "        self.num_steps = 0\n",
    "        self.state     = np.random.randint(2, size=self.num_bits)\n",
    "        self.target    = np.random.randint(2, size=self.num_bits)\n",
    "        return self.state, self.target\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise RESET\n",
    "        \n",
    "        self.state[action] = 1 - self.state[action]\n",
    "        \n",
    "        if self.num_steps > self.num_bits + 1:\n",
    "            self.done = True\n",
    "        self.num_steps += 1\n",
    "        \n",
    "        if np.sum(self.state == self.target) == self.num_bits:\n",
    "            self.done = True\n",
    "            return np.copy(self.state), 0, self.done, {}\n",
    "        else:\n",
    "            return np.copy(self.state), -1, self.done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size=256):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs,  hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "    \n",
    "    def forward(self, state, goal):\n",
    "        x = torch.cat([state, goal], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(model, state, goal, epsilon=0.1):\n",
    "    if random.random() < 0.1:\n",
    "        return random.randrange(env.num_bits)\n",
    "    \n",
    "    state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    goal  = torch.FloatTensor(goal).unsqueeze(0).to(device)\n",
    "    q_value = model(state, goal)\n",
    "    return q_value.max(1)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Mean Reward: %s. frame: %s' % (rewards[-1], frame_idx))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Q-learning TD Error</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_error(batch_size):\n",
    "    if batch_size > len(replay_buffer):\n",
    "        return None\n",
    "\n",
    "    state, action, reward, next_state, done, goal = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "    action     = torch.LongTensor(action).unsqueeze(1).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    goal       = torch.FloatTensor(goal).to(device)\n",
    "    mask       = torch.FloatTensor(1 - np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "    q_values = model(state, goal)\n",
    "    q_value  = q_values.gather(1, action)\n",
    "\n",
    "    next_q_values = target_model(next_state, goal)\n",
    "    target_action = next_q_values.max(1)[1].unsqueeze(1)\n",
    "    next_q_value  = target_model(next_state, goal).gather(1, target_action)\n",
    "\n",
    "    expected_q_value = reward + 0.99 * next_q_value * mask\n",
    "\n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DQN without Hindsight Experience Replay</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_bits = 11\n",
    "env = Env(num_bits)\n",
    "\n",
    "model        = Model(2 * num_bits, num_bits).to(device)\n",
    "target_model = Model(2 * num_bits, num_bits).to(device)\n",
    "update_target(model, target_model)\n",
    "\n",
    "#hyperparams:\n",
    "batch_size = 5\n",
    "new_goals  = 5\n",
    "max_frames = 200000\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "replay_buffer = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frame_idx = 0\n",
    "all_rewards = []\n",
    "losses = []\n",
    "\n",
    "while frame_idx < max_frames:\n",
    "    state, goal = env.reset()\n",
    "    done = False\n",
    "    episode = []\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = get_action(model, state, goal)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, done, goal)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 1000 == 0:\n",
    "            plot(frame_idx, [np.mean(all_rewards[i:i+100]) for i in range(0, len(all_rewards), 100)], losses)\n",
    "        \n",
    "    all_rewards.append(total_reward)\n",
    "    \n",
    "    loss = compute_td_error(batch_size)\n",
    "    if loss is not None: losses.append(loss.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Hindsight Experience Replay <a href=\"https://arxiv.org/abs/1707.01495\">[arxiv]</a></h1>\n",
    "<h2><a href=\"https://blog.openai.com/ingredients-for-robotics-research/#understandingher\">OpenAI Blog:</a> Understanding HER</h2>\n",
    "<p>To understand what HER does, let’s look at in the context of FetchSlide, a task where we need to learn to slide a puck across the table and hit a target. Our first attempt very likely will not be a successful one. Unless we get very lucky, the next few attempts will also likely not succeed. Typical reinforcement learning algorithms would not learn anything from this experience since they just obtain a constant reward (in this case: -1) that does not contain any learning signal.</p>\n",
    "\n",
    "<p>The key insight that HER formalizes is what humans do intuitively: Even though we have not succeeded at a specific goal, we have at least achieved a different one. So why not just pretend that we wanted to achieve this goal to begin with, instead of the one that we set out to achieve originally? By doing this substitution, the reinforcement learning algorithm can obtain a learning signal since it has achieved some goal; even if it wasn’t the one that we meant to achieve originally. If we repeat this process, we will eventually learn how to achieve arbitrary goals, including the goals that we really want to achieve.</p>\n",
    "\n",
    "<p>This approach lets us learn how to slide a puck across the table even though our reward is fully sparse and even though we may have never actually hit the desired goal early on. We call this technique Hindsight Experience Replay since it replays experience (a technique often used in off-policy RL algorithms like DQN and DDPG) with goals which are chosen in hindsight, after the episode has finished. HER can therefore be combined with any off-policy RL algorithm (for example, HER can be combined with DDPG, which we write as “DDPG + HER”).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict(achieved_goal:Box(3,), desired_goal:Box(3,), observation:Box(10,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num_bits = 11\n",
    "#env = Env(num_bits)\n",
    "env = gym.make(\"FetchReach-v1\")\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-bf88b4a4a0eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model        = Model(env.observation_space, env.action_space).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#target_model = Model(env.observation_space, env.action_space).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#hyperparams:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FetchReach-v1\")\n",
    "env.observation_space\n",
    "model        = Model(env.observation_space, env.action_space).to(device)\n",
    "target_model = Model(env.observation_space, env.action_space).to(device)\n",
    "update_target(model, target_model)\n",
    "\n",
    "#hyperparams:\n",
    "batch_size = 5\n",
    "new_goals  = 5\n",
    "max_frames = 200000\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "replay_buffer = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a2e109c26b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_frames' is not defined"
     ]
    }
   ],
   "source": [
    "frame_idx = 0\n",
    "all_rewards = []\n",
    "losses = []\n",
    "\n",
    "while frame_idx < max_frames:\n",
    "    state, goal = env.reset()\n",
    "    done = False\n",
    "    episode = []\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = get_action(model, state, goal)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state, reward, done, next_state, goal))\n",
    "        replay_buffer.push(state, action, reward, next_state, done, goal)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 10000 == 0:\n",
    "            plot(frame_idx, [np.mean(all_rewards[i:i+100]) for i in range(0, len(all_rewards), 100)], losses)\n",
    "        \n",
    "    all_rewards.append(total_reward)\n",
    "    \n",
    "    \n",
    "    new_episode = []\n",
    "\n",
    "    for state, reward, done, next_state, goal in episode:\n",
    "        for t in np.random.choice(num_bits, new_goals):\n",
    "            try:\n",
    "                episode[t]\n",
    "            except:\n",
    "                continue\n",
    "            new_goal = episode[t][-2]\n",
    "            if np.sum(next_state == new_goal) == num_bits:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = -1\n",
    "            replay_buffer.push(state, action, reward, next_state, done, new_goal)\n",
    "            new_episode.append((state, reward, done, next_state, new_goal))\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = compute_td_error(batch_size)\n",
    "    if loss is not None: losses.append(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL-a",
   "language": "python",
   "name": "rl-a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
